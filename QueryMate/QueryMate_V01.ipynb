{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/PKhosravi-CityTech/LightCnnRad/raw/main/Images/BioMindLogo.png\" alt=\"BioMind AI Lab Logo\" width=\"150\" height=\"150\" align=\"left\" style=\"margin-bottom: 40px;\"> **Repository Developed by Pegah Khosravi, Principal Investigator of the BioMind AI Lab**\n",
        "\n",
        "Welcome to this repository! This repository is a result of collaborative efforts from our dedicated team at the lab. We are committed to advancing the field of biomedical AI and pushing the boundaries of medical data analysis. Your interest and contributions to our work are greatly appreciated. For more information about our lab and ongoing projects, please visit the [BioMind AI Lab website](https://sites.google.com/view/biomind-ai-lab). Thank you for your interest and support!!"
      ],
      "metadata": {
        "id": "_v4s_w2Tyu5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QueryMate: A Custom LLM Powered by LlamaCpp\n",
        "\n",
        "Welcome to the repository of QueryMate, an interactive language model designed to provide intelligent responses to user queries. Built using the LlamaCpp framework, QueryMate leverages the powerful [Synthia-7B model](https://huggingface.co/TheBloke/SynthIA-7B-v2.0-16k-GGUF), finely tuned to handle a wide range of tasks. With a context window of 4096 tokens and optimized with 32 GPU layers, this model ensures efficient and accurate performance. The adjustable parameters, such as temperature and top_p, allow users to control the randomness and diversity of the model's output, making it versatile for various applications. The implementation includes a user-friendly interface with color-coded inputs and responses for enhanced readability. Explore the repository to learn more about the setup, usage, and customization options for QueryMate."
      ],
      "metadata": {
        "id": "iRk_FUeebSeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/PKhosravi-CityTech/BioMLScripts/raw/main/Images/QueryMate.png\" alt=\"QueryMate\" width=\"300\" height=\"300\" align=\"left\" style=\"margin-bottom: 40px;\">"
      ],
      "metadata": {
        "id": "xGVlMjsS7zOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing (NLP)** is a critical field within artificial intelligence focused on the interaction between computers and human language. Large Language Models (LLMs) represent a significant advancement in NLP, capable of understanding and generating human-like text based on vast amounts of data. Among the most renowned LLMs is ChatGPT, developed by OpenAI, which has demonstrated remarkable abilities in tasks such as conversation, text completion, and content generation."
      ],
      "metadata": {
        "id": "2zp-Bvv58vnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Getting Started\n",
        "\n",
        "To get started with QueryMate_V01, follow these steps:\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Make sure you are on GPU or TPU as this program takes 15 minutes on CPU, 5 minutes on GPU, and just a few seconds on TPU in Colab for each question.\n",
        "- Install the required libraries in your Colab:"
      ],
      "metadata": {
        "id": "3E4ucyG3qzH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "JdkNvxJhND_x",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cdac77f-86c8-4369-fef8-4ae705af9bb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
            "  Downloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.0 (from langchain-community)\n",
            "  Downloading langchain-0.2.3-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.0 (from langchain-community)\n",
            "  Downloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
            "  Downloading langsmith-0.1.76-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain-community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain-community)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading orjson-3.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.1)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain-community)\n",
            "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, multidict, jsonpointer, greenlet, frozenlist, async-timeout, yarl, typing-inspect, SQLAlchemy, marshmallow, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.3 langchain-community-0.2.4 langchain-core-0.2.5 langchain-text-splitters-0.2.1 langsmith-0.1.76 marshmallow-3.21.3 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.4 packaging-23.2 typing-inspect-0.9.0 yarl-1.9.4\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.78.tar.gz (50.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp310-cp310-linux_x86_64.whl size=3742448 sha256=0ffb1e5ad056dfeea7624b28daba8c501ec7de45f9bad37843cca0193b9256ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/c5/bd/3b1c20081bd71ce9d28b562573c97915c790bf1ef231879a61\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Version with Automated Model Downloading:\n",
        "\n",
        "In this new version of the code, we've added a feature that automatically downloads the model file directly to Google Drive if it isn't already present. This enhancement simplifies the setup process and ensures that the necessary model is readily available for use. The automated downloading process leverages the requests library to fetch the model from Hugging Face, ensuring a smooth and seamless experience. This addition is particularly useful for those who want to quickly get started without manually handling the model files."
      ],
      "metadata": {
        "id": "gzPzG0LgLVxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys  # Provides access to system-specific parameters and functions\n",
        "import textwrap  # Provides functions to format text\n",
        "from langchain.llms import LlamaCpp  # Import the LlamaCpp model from the langchain library\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Download model file if it does not exist\n",
        "import requests\n",
        "import os\n",
        "\n",
        "model_url = \"https://huggingface.co/TheBloke/SynthIA-7B-v2.0-16k-GGUF/resolve/main/synthia-7b-v2.0-16k.Q3_K_S.gguf\"\n",
        "local_model_path = \"/content/gdrive/MyDrive/ColabNotebooks_PegahKhosravi/BIO4450_Summer/synthia-7b-v2.0-16k.Q3_K_S.gguf\"\n",
        "\n",
        "if not os.path.exists(local_model_path):\n",
        "    print(\"Downloading model...\")\n",
        "    response = requests.get(model_url)\n",
        "    response.raise_for_status()\n",
        "    with open(local_model_path, \"wb\") as model_file:\n",
        "        model_file.write(response.content)\n",
        "    print(\"Model downloaded successfully.\")\n",
        "\n",
        "verbose = False  # Set verbose to False to reduce output logging\n",
        "\n",
        "# Initialize the LlamaCpp model and name it \"QueryMate\"\n",
        "querymate = LlamaCpp(\n",
        "    model_path=local_model_path,  # Path to the model file\n",
        "    n_ctx=4096,  # Set the context window size\n",
        "    n_gpu_layers=32,  # Set the number of GPU layers to use\n",
        "    n_batch=2048,  # Set the batch size\n",
        "    f16_kv=True,  # Enable 16-bit floating point precision for key-value memory\n",
        "    verbose=verbose,  # Use the verbose setting defined earlier\n",
        ")\n",
        "\n",
        "# Initialize parameters for controlling the model's response\n",
        "temperature = 0.2  # Controls the randomness of the model's output (lower value = less random)\n",
        "top_p = 0.1  # Controls the diversity of the output by sampling from the top p probability distribution (lower value = more conservative)\n",
        "\n",
        "# ANSI color codes for colored terminal output\n",
        "class Colors:\n",
        "    QUESTION = '\\033[94m'  # Blue\n",
        "    RESPONSE = '\\033[38;2;0;100;0m'  # Dark Green (RGB)\n",
        "    FEEDBACK = '\\033[38;2;255;165;0m'  # Orange (RGB)\n",
        "    RESET = '\\033[0m'  # Reset color to default\n",
        "\n",
        "def get_user_input(prompt):\n",
        "    \"\"\"Get input from the user.\"\"\"\n",
        "    try:\n",
        "        return input(prompt)  # Prompt the user for input\n",
        "    except EOFError:\n",
        "        return \"stop\"  # Return \"stop\" if an EOFError occurs (e.g., end of input)\n",
        "\n",
        "def adjust_parameters(feedback, temperature, top_p):\n",
        "    \"\"\"Adjust parameters based on user feedback.\"\"\"\n",
        "    if feedback == \"too random\":\n",
        "        temperature = max(0.1, temperature - 0.05)  # Decrease temperature to make output less random\n",
        "    elif feedback == \"too conservative\":\n",
        "        temperature = min(1.0, temperature + 0.05)  # Increase temperature to make output more random\n",
        "    return temperature, top_p  # Return the adjusted parameters\n",
        "\n",
        "def print_feedback_emoji(feedback):\n",
        "    \"\"\"Print a humorous emoji based on the feedback.\"\"\"\n",
        "    if feedback == \"good\":\n",
        "        print(\"ğŸ˜Š Great! Glad you liked it!\")\n",
        "    elif feedback == \"too random\":\n",
        "        print(\"ğŸ¤” Hmm, I'll try to be more focused.\")\n",
        "    elif feedback == \"too conservative\":\n",
        "        print(\"ğŸ˜´ Too boring? Let's spice it up!\")\n",
        "\n",
        "def main():\n",
        "    global temperature, top_p  # Use the global temperature and top_p variables\n",
        "\n",
        "    while True:\n",
        "        question = get_user_input(f\"{Colors.QUESTION}Ask me a question or type 'help' for options: {Colors.RESET}\")  # Prompt the user for a question (e.g., What does deep learning mean?)\n",
        "\n",
        "        if question == \"stop\":\n",
        "            print(\"Exiting the program.\")\n",
        "            break  # Exit the loop and stop the program\n",
        "        elif question == \"help\":\n",
        "            print(\"Options:\\n- Type 'stop' to exit\\n- Type 'feedback' to adjust response style\")\n",
        "            continue  # Continue to the next iteration of the loop\n",
        "        elif question == \"feedback\":\n",
        "            feedback = get_user_input(f\"{Colors.FEEDBACK}Enter feedback ('too random' or 'too conservative'): {Colors.RESET}\")\n",
        "            temperature, top_p = adjust_parameters(feedback, temperature, top_p)  # Adjust parameters based on feedback\n",
        "            print(f\"Parameters adjusted: temperature={temperature}, top_p={top_p}\")\n",
        "            print_feedback_emoji(feedback)\n",
        "            continue  # Continue to the next iteration of the loop\n",
        "\n",
        "        try:\n",
        "            print(\"Invoking model...\")\n",
        "            output = querymate.invoke(\n",
        "                question,  # The user's question\n",
        "                max_tokens=2048,  # Limit the maximum number of tokens in the output for faster response\n",
        "                temperature=temperature,  # Use the current temperature value\n",
        "                top_p=top_p  # Use the current top_p value\n",
        "            )\n",
        "            print(\"Model invoked successfully.\")\n",
        "            # Wrap the text output to 80 characters width for better readability\n",
        "            wrapped_output = textwrap.fill(output, width=80)\n",
        "            print(f\"\\n{Colors.RESPONSE}{wrapped_output}{Colors.RESET}\")\n",
        "\n",
        "            # Collect user feedback on the response\n",
        "            feedback = get_user_input(f\"{Colors.FEEDBACK}Rate the response ('good', 'too random', 'too conservative'): {Colors.RESET}\")\n",
        "            temperature, top_p = adjust_parameters(feedback, temperature, top_p)  # Adjust parameters based on feedback\n",
        "            print_feedback_emoji(feedback)\n",
        "\n",
        "            # Exit the program right after feedback to run faster\n",
        "            break  # Use break instead of sys.exit to avoid raising SystemExit\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")  # Print any errors that occur\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()  # Run the main function directly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYPq07CQJGGO",
        "outputId": "550ef64c-3fe1-4c7f-d089-cff7e508937a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Downloading model...\n",
            "Model downloaded successfully.\n",
            "\u001b[94mAsk me a question or type 'help' for options: \u001b[0mWhat does deep learning mean?\n",
            "Invoking model...\n",
            "Model invoked successfully.\n",
            "\n",
            "\u001b[38;2;0;100;0m  Deep learning is a subset of machine learning, which is a subset of artificial\n",
            "intelligence. Itâ€™s a way for computers to learn and make decisions without being\n",
            "explicitly programmed. Deep learning uses neural networks, which are made up of\n",
            "layers of nodes that can be trained on data to recognize patterns and make\n",
            "predictions.  How does deep learning work?  Deep learning works by training a\n",
            "neural network on a large amount of data. The data is fed into the network, and\n",
            "the network learns to recognize patterns in the data. The network then uses\n",
            "these patterns to make predictions or decisions.  What are some examples of deep\n",
            "learning?  Some examples of deep learning include:  1. Image recognition: Deep\n",
            "learning can be used to train a neural network to recognize objects in images.\n",
            "For example, a neural network could be trained on a large number of cat photos\n",
            "and then be able to identify cats in new photos. 2. Speech recognition: Deep\n",
            "learning can be used to train a neural network to recognize speech. The network\n",
            "is trained on a large amount of audio data and then can be used to transcribe\n",
            "spoken words into text. 3. Natural language processing: Deep learning can be\n",
            "used to train a neural network to understand natural language. The network is\n",
            "trained on a large amount of text data and then can be used to answer questions\n",
            "or generate responses. 4. Recommendation systems: Deep learning can be used to\n",
            "train a neural network to make recommendations based on user preferences. For\n",
            "example, a recommendation system could be trained on a userâ€™s browsing history\n",
            "and then recommend products or content that the user might like. 5. Self-driving\n",
            "cars: Deep learning is being used in self-driving cars to train neural networks\n",
            "to recognize objects and navigate safely. The network is trained on a large\n",
            "amount of data from cameras, radar, and other sensors, and then can be used to\n",
            "make decisions about how to steer the car.  What are some challenges with deep\n",
            "learning?  One challenge with deep learning is that it requires a lot of data to\n",
            "train the neural networks. The more data you have, the better the network will\n",
            "perform. However, collecting and labeling this data can be time-consuming and\n",
            "expensive.  Another challenge is that deep learning models are complex and can\n",
            "be difficult to interpret. Itâ€™s not always clear why a particular decision was\n",
            "made by the model, which can make it hard to trust the results.  Finally, deep\n",
            "learning models can be computationally intensive and require powerful hardware\n",
            "to train. This can make them expensive to run and can limit their accessibility\n",
            "to smaller organizations or individuals.\u001b[0m\n",
            "\u001b[38;2;255;165;0mRate the response ('good', 'too random', 'too conservative'): \u001b[0mgood\n",
            "ğŸ˜Š Great! Glad you liked it!\n"
          ]
        }
      ]
    }
  ]
}