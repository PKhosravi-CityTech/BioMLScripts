{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/PKhosravi-CityTech/LightCnnRad/raw/main/Images/BioMindLogo.png\" alt=\"BioMind AI Lab Logo\" width=\"150\" height=\"150\" align=\"left\" style=\"margin-bottom: 40px;\"> **Repository Developed by Pegah Khosravi, Principal Investigator of the BioMind AI Lab**\n",
        "\n",
        "Welcome to this repository! This repository is a result of collaborative efforts from our dedicated team at the lab. We are committed to advancing the field of biomedical AI and pushing the boundaries of medical data analysis. Your interest and contributions to our work are greatly appreciated. For more information about our lab and ongoing projects, please visit the [BioMind AI Lab website](https://sites.google.com/view/biomind-ai-lab). Thank you for your interest and support!!"
      ],
      "metadata": {
        "id": "_v4s_w2Tyu5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QueryMate: A Custom LLM Powered by LlamaCpp\n",
        "\n",
        "Welcome to the repository of QueryMate, an interactive language model designed to provide intelligent responses to user queries. Built using the LlamaCpp framework, QueryMate leverages the powerful [Synthia-7B model](https://huggingface.co/TheBloke/SynthIA-7B-v2.0-16k-GGUF), finely tuned to handle a wide range of tasks. With a context window of 4096 tokens and optimized with 32 GPU layers, this model ensures efficient and accurate performance. The adjustable parameters, such as temperature and top_p, allow users to control the randomness and diversity of the model's output, making it versatile for various applications. The implementation includes a user-friendly interface with color-coded inputs and responses for enhanced readability. Explore the repository to learn more about the setup, usage, and customization options for QueryMate."
      ],
      "metadata": {
        "id": "iRk_FUeebSeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/PKhosravi-CityTech/BioMLScripts/raw/main/Images/QueryMate.png\" alt=\"QueryMate\" width=\"300\" height=\"300\" align=\"left\" style=\"margin-bottom: 40px;\">"
      ],
      "metadata": {
        "id": "xGVlMjsS7zOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing (NLP)** is a critical field within artificial intelligence focused on the interaction between computers and human language. Large Language Models (LLMs) represent a significant advancement in NLP, capable of understanding and generating human-like text based on vast amounts of data. Among the most renowned LLMs is ChatGPT, developed by OpenAI, which has demonstrated remarkable abilities in tasks such as conversation, text completion, and content generation."
      ],
      "metadata": {
        "id": "2zp-Bvv58vnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QueryMate is an AI-powered question-answering system built using the LlamaCpp model from the langchain library. This system is designed to provide precise answers to user questions and allows for adjustable response parameters based on user feedback.\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "To get started with QueryMate, follow these steps:\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Python 3.6 or later\n",
        "- Make sure you are on GPU or TPU as this program takes 15 minutes on CPU, 5 minutes on GPU, and just a few seconds on TPU in Colab for each question.\n",
        "- Install the required libraries in your Colab:\n",
        "\n",
        "  ```bash\n",
        "  !pip install langchain-community\n",
        "  !pip install llama-cpp-python\n",
        "\n",
        "  ```\n",
        "\n",
        "### Download the SynthIA-7B-v2.0-16k model\n",
        "\n",
        "You need to download the SynthIA-7B-v2.0-16k model file from Hugging Face.\n",
        "Visit the following link and download the latest or the second latest version of the file\n",
        "named synthia-7b-v2.0-16k.Q3_K_S.gguf and then upload it to your gdrive:\n",
        "\n",
        "  ```bash\n",
        "  https://huggingface.co/TheBloke/SynthIA-7B-v2.0-16k-GGUF\n",
        "\n",
        "  ```\n",
        "\n",
        "### Download the QueryMate model\n",
        "  ```bash\n",
        "  https://github.com/PKhosravi-CityTech/BioMLScripts/blob/main/QueryMate/QueryMate.ipynb\n",
        "\n",
        "  ```"
      ],
      "metadata": {
        "id": "3E4ucyG3qzH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "JdkNvxJhND_x",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae642fa-1049-42ca-e6c5-fb39372cb6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
            "  Downloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.0 (from langchain-community)\n",
            "  Downloading langchain-0.2.3-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.0 (from langchain-community)\n",
            "  Downloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
            "  Downloading langsmith-0.1.75-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain-community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain-community)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.1)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain-community)\n",
            "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, multidict, jsonpointer, greenlet, frozenlist, async-timeout, yarl, typing-inspect, SQLAlchemy, marshmallow, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.3 langchain-community-0.2.4 langchain-core-0.2.5 langchain-text-splitters-0.2.1 langsmith-0.1.75 marshmallow-3.21.3 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0 yarl-1.9.4\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.77.tar.gz (50.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.77-cp310-cp310-linux_x86_64.whl size=3726679 sha256=cd0a97b2706ab17d4c128db6f36046820cc709a0a9e09fbfc7eaa23c3cb740c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/55/a1/6d6c2ef6fed3ef054b4170d8bcd05a09e6dc971db7fad955ff\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "_zlyfvGaW7vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ba5d33-1c6c-469f-8261-3a81c7b42359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys  # Provides access to system-specific parameters and functions\n",
        "import textwrap  # Provides functions to format text\n",
        "from langchain.llms import LlamaCpp  # Import the LlamaCpp model from the langchain library\n",
        "\n",
        "verbose = False  # Set verbose to False to reduce output logging\n",
        "\n",
        "# Initialize the LlamaCpp model and name it \"QueryMate\"\n",
        "querymate = LlamaCpp(\n",
        "    model_path=\"/content/gdrive/MyDrive/ColabNotebooks_PegahKhosravi/synthia-7b-v2.0-16k.Q3_K_S.gguf\",  # Path to the model file\n",
        "    n_ctx=4096,  # Set the context window size\n",
        "    n_gpu_layers=32,  # Set the number of GPU layers to use\n",
        "    n_batch=2048,  # Set the batch size\n",
        "    f16_kv=True,  # Enable 16-bit floating point precision for key-value memory\n",
        "    verbose=verbose,  # Use the verbose setting defined earlier\n",
        ")\n",
        "\n",
        "# Initialize parameters for controlling the model's response\n",
        "temperature = 0.2  # Controls the randomness of the model's output (lower value = less random)\n",
        "top_p = 0.1  # Controls the diversity of the output by sampling from the top p probability distribution (lower value = more conservative)\n",
        "\n",
        "# ANSI color codes for colored terminal output\n",
        "class Colors:\n",
        "    QUESTION = '\\033[94m'  # Blue\n",
        "    RESPONSE = '\\033[38;2;0;100;0m'  # Dark Green (RGB)\n",
        "    FEEDBACK = '\\033[38;2;255;165;0m'  # Orange (RGB)\n",
        "    RESET = '\\033[0m'  # Reset color to default\n",
        "\n",
        "def get_user_input(prompt):\n",
        "    \"\"\"Get input from the user.\"\"\"\n",
        "    try:\n",
        "        return input(prompt)  # Prompt the user for input\n",
        "    except EOFError:\n",
        "        return \"stop\"  # Return \"stop\" if an EOFError occurs (e.g., end of input)\n",
        "\n",
        "def adjust_parameters(feedback, temperature, top_p):\n",
        "    \"\"\"Adjust parameters based on user feedback.\"\"\"\n",
        "    if feedback == \"too random\":\n",
        "        temperature = max(0.1, temperature - 0.05)  # Decrease temperature to make output less random\n",
        "    elif feedback == \"too conservative\":\n",
        "        temperature = min(1.0, temperature + 0.05)  # Increase temperature to make output more random\n",
        "    return temperature, top_p  # Return the adjusted parameters\n",
        "\n",
        "def print_feedback_emoji(feedback):\n",
        "    \"\"\"Print a humorous emoji based on the feedback.\"\"\"\n",
        "    if feedback == \"good\":\n",
        "        print(\"😊 Great! Glad you liked it!\")\n",
        "    elif feedback == \"too random\":\n",
        "        print(\"🤔 Hmm, I'll try to be more focused.\")\n",
        "    elif feedback == \"too conservative\":\n",
        "        print(\"😴 Too boring? Let's spice it up!\")\n",
        "\n",
        "def main():\n",
        "    global temperature, top_p  # Use the global temperature and top_p variables\n",
        "\n",
        "    while True:\n",
        "        question = get_user_input(f\"{Colors.QUESTION}Ask me a question or type 'help' for options: {Colors.RESET}\")  # Prompt the user for a question (e.g. What does deep learning mean?)\n",
        "\n",
        "        if question == \"stop\":\n",
        "            print(\"Exiting the program.\")\n",
        "            break  # Exit the loop and stop the program\n",
        "        elif question == \"help\":\n",
        "            print(\"Options:\\n- Type 'stop' to exit\\n- Type 'feedback' to adjust response style\")\n",
        "            continue  # Continue to the next iteration of the loop\n",
        "        elif question == \"feedback\":\n",
        "            feedback = get_user_input(f\"{Colors.FEEDBACK}Enter feedback ('too random' or 'too conservative'): {Colors.RESET}\")\n",
        "            temperature, top_p = adjust_parameters(feedback, temperature, top_p)  # Adjust parameters based on feedback\n",
        "            print(f\"Parameters adjusted: temperature={temperature}, top_p={top_p}\")\n",
        "            print_feedback_emoji(feedback)\n",
        "            continue  # Continue to the next iteration of the loop\n",
        "\n",
        "        try:\n",
        "            print(\"Invoking model...\")\n",
        "            output = querymate.invoke(\n",
        "                question,  # The user's question\n",
        "                max_tokens=2048,  # Limit the maximum number of tokens in the output for faster response\n",
        "                temperature=temperature,  # Use the current temperature value\n",
        "                top_p=top_p  # Use the current top_p value\n",
        "            )\n",
        "            print(\"Model invoked successfully.\")\n",
        "            # Wrap the text output to 80 characters width for better readability\n",
        "            wrapped_output = textwrap.fill(output, width=80)\n",
        "            print(f\"\\n{Colors.RESPONSE}{wrapped_output}{Colors.RESET}\")\n",
        "\n",
        "            # Collect user feedback on the response\n",
        "            feedback = get_user_input(f\"{Colors.FEEDBACK}Rate the response ('good', 'too random', 'too conservative'): {Colors.RESET}\")\n",
        "            temperature, top_p = adjust_parameters(feedback, temperature, top_p)  # Adjust parameters based on feedback\n",
        "            print_feedback_emoji(feedback)\n",
        "\n",
        "            # Exit the program right after feedback to run faster\n",
        "            break  # Use break instead of sys.exit to avoid raising SystemExit\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")  # Print any errors that occur\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()  # Run the main function directly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fdmj8SwsnSe",
        "outputId": "b926a2a2-c9d6-4c39-b168-170b30ac97b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mAsk me a question or type 'help' for options: \u001b[0mWhat does deep learning mean?\n",
            "Invoking model...\n",
            "Model invoked successfully.\n",
            "\n",
            "\u001b[38;2;0;100;0m  Deep learning is a subset of machine learning in artificial intelligence that\n",
            "has networks capable of learning representations or features of data with\n",
            "multiple levels of abstraction. Deep learning algorithms are based on a set of\n",
            "layers, such as an input layer, hidden layers and an output layer. The hidden\n",
            "layers can further be broken down into several sub-layers.  Deep learning is\n",
            "used in various applications, including image recognition, natural language\n",
            "processing (NLP), speech recognition, and more. It has been successful in these\n",
            "areas because it can learn complex patterns from large amounts of data.  In deep\n",
            "learning, the network learns by adjusting the weights of its connections based\n",
            "on the error between the predicted output and the actual output. This process is\n",
            "known as backpropagation. The deeper the layers are, the more abstract the\n",
            "features that can be learned.  Deep learning has been successful in many areas\n",
            "because it can learn complex patterns from large amounts of data. It has been\n",
            "used to recognize objects in images, understand natural language, and even\n",
            "generate new images or text. Deep learning is a powerful tool for solving\n",
            "complex problems, but it requires a lot of computational power and a large\n",
            "amount of labeled data to train the network effectively.\u001b[0m\n",
            "\u001b[38;2;255;165;0mRate the response ('good', 'too random', 'too conservative'): \u001b[0mgood\n",
            "😊 Great! Glad you liked it!\n"
          ]
        }
      ]
    }
  ]
}